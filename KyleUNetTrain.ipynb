{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f82826",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "966b2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from itkwidgets import view\n",
    "\n",
    "import model as md\n",
    "import dataset as dtst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea9fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_tensor_type('torch.cuda.DoubleTensor')\n",
    "print(f\"{device = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef3a16",
   "metadata": {},
   "source": [
    "## Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c559cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params 464849, # conv layers 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet64(\n",
       "  (conv1_8): Conv3d(1, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv8_8): Conv3d(8, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv8_16): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv16_16): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv16_32): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv32_32): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv32_64): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv64_64): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv64_32): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv32_16): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv16_8): Conv3d(16, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv8_1): Conv3d(8, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (up_conv64_32): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "  (up_conv32_16): ConvTranspose3d(32, 16, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "  (up_conv16_8): ConvTranspose3d(16, 8, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = md.UNet64()\n",
    "model.train()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437dba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "net.load_state_dict(torch.load(\"UNet64_cube_sz_64_p_05.pt\"))\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361824c6",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a584c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dtst.UnetDataset(root_dir=\"/home/sci/kyle.anderson/lymph_nodes/Dataset\", patch_size=64, min_probability=0.5)\n",
    "train_set, test_set = random_split(dataset,\n",
    "                                   [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)],\n",
    "                                   generator=torch.Generator('cuda'))\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "val_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bda4b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa29a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_loader, model, loss_fn, optimizer, log_file=None):\n",
    "    epoch_loss = 0.0\n",
    "    size = len(train_loader.dataset)\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            input_data = batch[\"img\"].cuda()\n",
    "            pred = model(input_data)\n",
    "            truth = batch[\"mask\"].cuda()\n",
    "            \n",
    "            loss = loss_fn(pred, truth)\n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % (size // 20) == 0:\n",
    "            print(f\"Loss: {loss.item():06.5f}\\t[{idx*len(batch['name']):3d}/{size:3d}]\")\n",
    "        \n",
    "    if log_file is not None:\n",
    "        time_stamp = time.ctime(time.time())\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"Time: {time_stamp}\\tLoss = {epoch_loss:.5f}\\n\")\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880eff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(test_loader, model, loss_fn, log_file=None):\n",
    "    size = len(test_loader.dataset)\n",
    "    num_batches = len(test_loader)\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            pred = model(batch[\"img\"].cuda())\n",
    "            test_loss += loss_fn(pred, batch[\"mask\"].cuda()).item()\n",
    "            \n",
    "    avg_loss = test_loss / num_batches\n",
    "    print(f\"Average test loss: {avg_loss:.5f}\")\n",
    "    \n",
    "    if log_file is not None:\n",
    "        time_stamp = time.ctime(time.time())\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"Time: {time_stamp}\\tAverage loss = {avg_loss:.5f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb30d6",
   "metadata": {},
   "source": [
    "# Only run the following section of you are trying to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176ae927",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.10450\t[  0/332]\n",
      "Loss: 0.11286\t[ 64/332]\n",
      "Loss: 0.10182\t[128/332]\n",
      "Loss: 0.10775\t[192/332]\n",
      "Loss: 0.10018\t[256/332]\n",
      "Loss: 0.09956\t[320/332]\n",
      "Average test loss: 0.11296\n",
      "Epoch elapsed time:  0h:38m: 7.673s\n",
      "Epoch 42 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.10283\t[  0/332]\n",
      "Loss: 0.11106\t[ 64/332]\n",
      "Loss: 0.10017\t[128/332]\n",
      "Loss: 0.10617\t[192/332]\n",
      "Loss: 0.09837\t[256/332]\n",
      "Loss: 0.09793\t[320/332]\n",
      "Average test loss: 0.11135\n",
      "Epoch elapsed time:  0h:37m:43.960s\n",
      "Epoch 43 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.10118\t[  0/332]\n",
      "Loss: 0.10930\t[ 64/332]\n",
      "Loss: 0.09855\t[128/332]\n",
      "Loss: 0.10463\t[192/332]\n",
      "Loss: 0.09659\t[256/332]\n",
      "Loss: 0.09633\t[320/332]\n",
      "Average test loss: 0.10976\n",
      "Epoch elapsed time:  0h:37m:25.144s\n",
      "Epoch 44 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.09956\t[  0/332]\n",
      "Loss: 0.10758\t[ 64/332]\n",
      "Loss: 0.09696\t[128/332]\n",
      "Loss: 0.10312\t[192/332]\n",
      "Loss: 0.09486\t[256/332]\n",
      "Loss: 0.09478\t[320/332]\n",
      "Average test loss: 0.10819\n",
      "Epoch elapsed time:  0h:36m:49.533s\n",
      "Epoch 45 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.09796\t[  0/332]\n",
      "Loss: 0.10590\t[ 64/332]\n",
      "Loss: 0.09541\t[128/332]\n",
      "Loss: 0.10165\t[192/332]\n",
      "Loss: 0.09316\t[256/332]\n",
      "Loss: 0.09326\t[320/332]\n",
      "Average test loss: 0.10665\n",
      "Epoch elapsed time:  0h:36m:41.635s\n",
      "Epoch 46 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.09638\t[  0/332]\n",
      "Loss: 0.10425\t[ 64/332]\n",
      "Loss: 0.09390\t[128/332]\n",
      "Loss: 0.10021\t[192/332]\n",
      "Loss: 0.09150\t[256/332]\n",
      "Loss: 0.09178\t[320/332]\n",
      "Average test loss: 0.10514\n",
      "Epoch elapsed time:  0h:37m:43.813s\n",
      "Epoch 47 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.09484\t[  0/332]\n",
      "Loss: 0.10264\t[ 64/332]\n",
      "Loss: 0.09242\t[128/332]\n",
      "Loss: 0.09880\t[192/332]\n",
      "Loss: 0.08988\t[256/332]\n",
      "Loss: 0.09033\t[320/332]\n",
      "Average test loss: 0.10367\n",
      "Epoch elapsed time:  0h:37m:32.194s\n",
      "Epoch 48 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.09334\t[  0/332]\n",
      "Loss: 0.10107\t[ 64/332]\n",
      "Loss: 0.09098\t[128/332]\n",
      "Loss: 0.09743\t[192/332]\n",
      "Loss: 0.08830\t[256/332]\n",
      "Loss: 0.08893\t[320/332]\n",
      "Average test loss: 0.10224\n",
      "Epoch elapsed time:  0h:37m:30.731s\n",
      "Epoch 49 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.09188\t[  0/332]\n",
      "Loss: 0.09954\t[ 64/332]\n",
      "Loss: 0.08957\t[128/332]\n",
      "Loss: 0.09608\t[192/332]\n",
      "Loss: 0.08676\t[256/332]\n",
      "Loss: 0.08755\t[320/332]\n",
      "Average test loss: 0.10087\n",
      "Epoch elapsed time:  0h:37m:22.077s\n",
      "Epoch 50 of 50\n",
      "----------------------------------------\n",
      "Loss: 0.09047\t[  0/332]\n",
      "Loss: 0.09805\t[ 64/332]\n",
      "Loss: 0.08820\t[128/332]\n",
      "Loss: 0.09477\t[192/332]\n",
      "Loss: 0.08526\t[256/332]\n",
      "Loss: 0.08621\t[320/332]\n",
      "Average test loss: 0.09953\n",
      "Epoch elapsed time:  0h:37m:26.259s\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "start_epoch, end_epoch = 40, 50\n",
    "\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    print(f\"Epoch {epoch+1} of {end_epoch}\\n{'-'*40}\")\n",
    "    start_time = time.time()\n",
    "    epoch_loss = train_loop(\n",
    "        train_loader, net, criterion, optimizer, \"UNet64_cube_sz_64_p_05_train_loss.txt\")\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    test_loop(test_loader, net, criterion, \"UNet64_cube_sz_64_p_05_test_loss.txt\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Epoch elapsed time: {int(elapsed_time//3600):2d}h:{int((elapsed_time%3600)//60):2d}m:{elapsed_time%60:6.3f}s\")\n",
    "    \n",
    "    torch.save(net.state_dict(), \"UNet64_cube_sz_64_p_05.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2089e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(dataset, model, num_samples):\n",
    "    predictions = {}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in range(num_samples):\n",
    "            pred = model(dataset[sample][\"img\"].cuda().unsqueeze(0))\n",
    "            predictions[dataset[sample][\"name\"]] = pred.cpu().detach().numpy().squeeze()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0677cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_inference(dataset, net, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ddc2ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcedd90048a45b383b8283e40cce146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[], gradient_opacity=0.22, interpolation=False, point_sets=[], rendered_image=<itk.itkImagePâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_list = list(predictions.values())\n",
    "mask_0 = dataset[0][\"mask\"]\n",
    "view(preds_list[0], mask_0.squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
