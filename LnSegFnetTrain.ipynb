{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dataset import *\n",
    "from model import *\n",
    "from loss import *\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "from itkwidgets import view\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode='gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode=='gpu':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # after switch device, you need restart the kernel\n",
    "#     torch.cuda.set_device(1)\n",
    "    torch.set_default_tensor_type('torch.cuda.DoubleTensor')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For classifications(segmentation=voxel-wise classification), `F.softmax(output, dim=1)` is very necessary at the end of the model, as it constraints the output into a probability, or you may have negative value that you also have no clue where it comes from.\n",
    "2. The numerator in dice loss for each category is very much like the cross entropy: a softmax vector inner product with a one-hot vector - only the value at where one is matters.\n",
    "2. For segmentation, use dice loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume:False, save_model:True\n"
     ]
    }
   ],
   "source": [
    "resume = False\n",
    "save_model = True\n",
    "print(f'resume:{resume}, save_model:{save_model}')\n",
    "output_dir = 'Models/FNet'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params 176118, # conv layers 40\n",
      "Starting from iteration 0 to iteration 1001\n"
     ]
    }
   ],
   "source": [
    "epoch_loss_list = []\n",
    "epoch_num = 1001\n",
    "start_epoch_num = 7\n",
    "batch_size = 6\n",
    "learning_rate = 8e0\n",
    "\n",
    "model = FNet()\n",
    "model.train()\n",
    "if mode=='gpu':\n",
    "    model.cuda()\n",
    "net = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "\n",
    "dataset = FnetDataset(root_dir='/home/sci/hdai/Projects/Dataset/LymphNodes')\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "if resume:\n",
    "    checkpoint = torch.load(f'{output_dir}/epoch_{start_epoch_num-1}_checkpoint.pth.tar')    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    with open(f'{output_dir}/loss.txt', 'a') as f:\n",
    "        f.write(f'From {start_epoch_num} to {epoch_num+start_epoch_num}\\n')\n",
    "        f.write(f'BCE; Adadelta, lr={learning_rate}; batch size: {batch_size}\\n')\n",
    "else:\n",
    "    start_epoch_num = 0  \n",
    "    \n",
    "    with open(f'{output_dir}/loss.txt', 'w+') as f:\n",
    "        f.write(f'From {start_epoch_num} to {epoch_num+start_epoch_num}\\n')\n",
    "        f.write(f'BCE; Adadelta: lr={learning_rate}; batch size: {batch_size}\\n')\n",
    "    \n",
    "print(f'Starting from iteration {start_epoch_num} to iteration {epoch_num+start_epoch_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1001 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:51, 51.85s/it]\u001b[A\n",
      "2it [01:35, 46.84s/it]\u001b[A\n",
      "3it [02:27, 49.34s/it]\u001b[A\n",
      "4it [03:11, 47.32s/it]\u001b[A\n",
      "5it [03:55, 46.06s/it]\u001b[A\n",
      "6it [04:40, 45.61s/it]\u001b[A\n",
      "7it [05:38, 49.59s/it]\u001b[A\n",
      "8it [06:27, 49.68s/it]\u001b[A\n",
      "9it [07:15, 48.96s/it]\u001b[A\n",
      "10it [08:06, 49.58s/it]\u001b[A\n",
      "11it [08:47, 46.89s/it]\u001b[A\n",
      "12it [09:15, 41.27s/it]\u001b[A\n",
      "13it [09:55, 41.03s/it]\u001b[A\n",
      "14it [11:03, 49.04s/it]\u001b[A\n",
      "15it [11:51, 47.45s/it]\u001b[A\n",
      "  0%|          | 1/1001 [11:51<197:44:15, 711.86s/it]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 innerdomain loss: 1.5806993324481435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:29, 29.42s/it]\u001b[A\n",
      "2it [00:59, 30.05s/it]\u001b[A\n",
      "3it [01:31, 30.61s/it]\u001b[A\n",
      "4it [02:02, 31.00s/it]\u001b[A\n",
      "5it [02:35, 31.63s/it]\u001b[A\n",
      "6it [03:08, 32.14s/it]\u001b[A\n",
      "7it [03:41, 32.33s/it]\u001b[A\n",
      "8it [04:13, 32.22s/it]\u001b[A\n",
      "9it [04:44, 31.78s/it]\u001b[A\n",
      "10it [05:16, 31.94s/it]\u001b[A\n",
      "11it [05:48, 32.11s/it]\u001b[A\n",
      "12it [06:18, 31.23s/it]\u001b[A\n",
      "13it [06:50, 31.68s/it]\u001b[A\n",
      "14it [07:21, 31.47s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(start_epoch_num, start_epoch_num+epoch_num)):\n",
    "    epoch_loss = 0\n",
    "            \n",
    "    for i, batched_sample in tqdm(enumerate(dataloader)):\n",
    "        '''innerdomain backpropagate'''\n",
    "#         print(i)\n",
    "        input0 = batched_sample['img0'].double()#.to(device)\n",
    "        input1 = batched_sample['img1'].double()#.to(device)\n",
    "        input2 = batched_sample['img2'].double()#.to(device)\n",
    "        input3 = batched_sample['img3'].double()#.to(device)\n",
    "#         print(input.shape)\n",
    "        input0.requires_grad = True\n",
    "        input1.requires_grad = True\n",
    "        input2.requires_grad = True\n",
    "        input3.requires_grad = True\n",
    "        # u_pred: [batch_size, *data_shape, feature_num] = [1, 5, ...]\n",
    "        output_pred = net(input0,input1,input2,input3)\n",
    "        output_true = batched_sample['mask']#.to(device)#.double()\n",
    "#         print(output_pred.shape, output_true.shape)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "#         loss = criterion(output_pred, output_true.squeeze())\n",
    "        loss = criterion(output_pred, output_true.double())\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    with open(f'{output_dir}/loss.txt', 'a') as f:\n",
    "        f.write(f'{epoch_loss}\\n')\n",
    "    \n",
    "    print(f'epoch {epoch} loss: {epoch_loss}')#, norm: {torch.norm(f_pred,2)**2}\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    if epoch%1==0:       \n",
    "        if save_model:\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "            }, f'{output_dir}/epoch_{epoch}_checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(output_true[1,0].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Innerdomain loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('BCE loss')\n",
    "plt.plot(epoch_loss_list)\n",
    "plt.savefig(f'{output_dir}/adadelta_loss_{learning_rate}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch17",
   "language": "python",
   "name": "pytorch17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
